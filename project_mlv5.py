# -*- coding: utf-8 -*-
"""Project MLv5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uMrlfovQzFqf1_aAqbyz8jfgsC7n88fz
"""

import pandas as pd
import numpy as np

import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split

"""# **Data Processing**"""

from google.colab import drive
drive.mount("/content/drive")

"""## Import Data

!! missing 2019 and some files
"""

def createraw () :

    Jan_Apr10 = pd.read_excel("/content/drive/My Drive/Received Data/2010_Jan_Apr.xlsx")
    May_Aug10 = pd.read_excel("/content/drive/My Drive/Received Data/2010_May_Aug.xlsx")
    Sept_Dec10 = pd.read_excel("/content/drive/My Drive/Received Data/2010_Sept_Dec.xlsx")
    Over1Million10 = pd.read_excel("/content/drive/My Drive/Received Data/2010_Over_1_Million.xlsx")

    Jan_Apr11 = pd.read_excel("/content/drive/My Drive/Received Data/2011_Jan_Apr.xlsx")
    May_Aug11 = pd.read_excel("/content/drive/My Drive/Received Data/2011_May_Aug.xlsx")
    Sept_Dec11 = pd.read_excel("/content/drive/My Drive/Received Data/2011_Sept_Dec.xlsx")
    Over1Million11 = pd.read_excel("/content/drive/My Drive/Received Data/2011_Over_1_Million.xlsx")

    Jan_Apr12 = pd.read_excel("/content/drive/My Drive/Received Data/2012_Jan_Apr.xlsx")
    May_Aug12 = pd.read_excel("/content/drive/My Drive/Received Data/2012_May_Aug.xlsx")
    Sept_Dec12 = pd.read_excel("/content/drive/My Drive/Received Data/2012_Sept_Dec.xlsx")
    Over1Million12 = pd.read_excel("/content/drive/My Drive/Received Data/2012_Over_1_Million.xlsx")

    Jan_Apr13 = pd.read_excel("/content/drive/My Drive/Received Data/2013_Jan_Apr.xlsx")
    May_Aug13 = pd.read_excel("/content/drive/My Drive/Received Data/2013_May_Aug.xlsx")
    Sept_Dec13 = pd.read_excel("/content/drive/My Drive/Received Data/2013_Sept_Dec.xlsx")
    Over1Million13 = pd.read_excel("/content/drive/My Drive/Received Data/2013_Over_1_Million.xlsx")

    Jan_Apr14 = pd.read_excel("/content/drive/My Drive/Received Data/2014_Jan_Apr.xlsx")
    May_Aug14 = pd.read_excel("/content/drive/My Drive/Received Data/2014_May_Aug.xlsx")
    Sept_Dec14 = pd.read_excel("/content/drive/My Drive/Received Data/2014_Sept_Dec.xlsx")
    Over1Million14 = pd.read_excel("/content/drive/My Drive/Received Data/2014_Over_1_million.xlsx")

    Jan_Apr15 = pd.read_excel("/content/drive/My Drive/Received Data/2015_Jan_Apr.xlsx")
    May_Aug15 = pd.read_excel("/content/drive/My Drive/Received Data/2015_May_Aug.xlsx")
    Sept_Dec15 = pd.read_excel("/content/drive/My Drive/Received Data/2015_Sept_Dec.xlsx")
    Over1Million15 = pd.read_excel("/content/drive/My Drive/Received Data/2015_Over_1_Million.xlsx")

    Jan_Apr16 = pd.read_excel("/content/drive/My Drive/Received Data/2016_Jan_Apr.xlsx")
    May_Aug16 = pd.read_excel("/content/drive/My Drive/Received Data/2016_May_Aug.xlsx")
    Sept_Dec16 = pd.read_excel("/content/drive/My Drive/Received Data/2016_Sept_Dec.xlsx")
    Over1Million16 = pd.read_excel("/content/drive/My Drive/Received Data/2016_Over_1_Million.xlsx")

    Jan_Apr17 = pd.read_excel("/content/drive/My Drive/Received Data/2017_Jan_Apr.xlsx")
    May_Aug17 = pd.read_excel("/content/drive/My Drive/Received Data/2017_May_Aug.xlsx")
    Sept_Dec17 = pd.read_excel("/content/drive/My Drive/Received Data/2017_Sept_Dec.xlsx")
    Over1Million17 = pd.read_excel("/content/drive/My Drive/Received Data/2017_Over_1_Million.xlsx")

    Jan_Apr18 = pd.read_excel("/content/drive/My Drive/Received Data/2018_Jan_Apr.xlsx")
    May_Aug18 = pd.read_excel("/content/drive/My Drive/Received Data/2018_May_Aug.xlsx")
    Sept_Dec18 = pd.read_excel("/content/drive/My Drive/Received Data/2018_Sept_Dec.xlsx")
    Over1Million18 = pd.read_excel("/content/drive/My Drive/Received Data/2018_Over_1_Million.xlsx")

    Jan_Dec19 = pd.read_excel("/content/drive/My Drive/Received Data/2019_Jan_Dec.xlsx")
    Over1Million19 = pd.read_excel("/content/drive/My Drive/Received Data/2019_Over_1_Million.xlsx")

    raw_tmp = pd.concat([Jan_Apr10,Jan_Apr11,Jan_Apr12,Jan_Apr13,Jan_Apr14,Jan_Apr15,
                 Jan_Apr16,Jan_Apr17,Jan_Apr18,May_Aug10,May_Aug11,May_Aug12,
                 May_Aug13,May_Aug14,May_Aug15,May_Aug16,May_Aug17,May_Aug18,
                 Sept_Dec10,Sept_Dec11,Sept_Dec12,Sept_Dec13,Sept_Dec14,Sept_Dec15,
                 Sept_Dec16,Sept_Dec17,Sept_Dec18,Jan_Dec19,Over1Million10,Over1Million11,
                 Over1Million12,Over1Million13,Over1Million14,
                 Over1Million15,
                 Over1Million16,Over1Million17,Over1Million18,Over1Million19],ignore_index=True)
    
    raw = raw_tmp.loc[:,["ML #","PicCount","Status","Sold Price","Price","List Price","Original Price","DOM","TotFlArea","Bylaw Restrictions","List Date","Sold Date","Tot BR",
                         "Tot Baths","Yr Blt","Age","Locker","TotalPrkng","StratMtFee","Address","S/A","TypeDwel","Cumulative DOMLS",
                         "Cumulative DOM","Floor Area Fin - Total","Foundation","Full Baths","Half Baths","Lot Sz (Sq.Ft.)","No. Floor Levels",
                         "Postal Code","Stories in Building","Tot Units in Strata Plan","Zoning","View","Previous Price Sys","Public Remarks"]]
    
    return raw

raw = createraw()

def changetype (raw) :
    # using dictionary to convert specific columns 
    convert_dict = {"ML #": str,
                "PicCount": int, 
                "Status": str,
                "Sold Price": int,
                "Price":int,
                "List Price":float,
                "Original Price":float,
                "DOM": float,
                "TotFlArea": float,
                "Bylaw Restrictions": str,
                "List Date": str,
                "Sold Date": str,
                "Tot BR": float,
                "Tot Baths":float,
                "Yr Blt":float,
                "Age":float,
                "Locker":str,
                "TotalPrkng":str,
                "StratMtFee":float,
                "Address":str,
                "S/A": str,
                "TypeDwel":str,
                "Cumulative DOMLS":float,
                "Cumulative DOM":float,
                "Floor Area Fin - Total":float,
                "Foundation":str,
                "Full Baths":float,
                "Half Baths":float,
                "Lot Sz (Sq.Ft.)":float,
                "No. Floor Levels":float,
                "Postal Code":str,
                "Stories in Building":str,
                "Tot Units in Strata Plan":str,
                "Zoning":str,
                "View":str,
                "Previous Price Sys":str,
                "Public Remarks":str
               } 
    raw = raw.astype(convert_dict)  
    return raw

def daystosell (raw):
    raw["year_SoldDate"] = raw["Sold Date"].str[:4].astype(float)
    raw["month_SoldDate"] = raw["Sold Date"].str[5:7].astype(float)
    raw["day_SoldDate"] = raw["Sold Date"].str[8:].astype(float)

    raw["year_ListDate"] = raw["List Date"].str[:4].astype(float)
    raw["month_ListDate"] = raw["List Date"].str[5:7].astype(float)
    raw["day_ListDate"] = raw["List Date"].str[8:].astype(float)
    
    raw["daystosell"] = (raw["year_SoldDate"]-raw["year_ListDate"])*365 + \
    (raw["month_SoldDate"]- raw["month_ListDate"])*30 + raw["day_SoldDate"]-raw["day_ListDate"]

    raw["day_month_year"] = raw["year_ListDate"]*365 + raw["month_ListDate"]*30 + raw["day_ListDate"]
    
    del raw["year_SoldDate"]
    del raw["month_SoldDate"]
    del raw["day_SoldDate"]

    del raw["year_ListDate"]
    del raw["month_ListDate"]
    del raw["day_ListDate"]
    
    raw["daystosell_bin"] = round(raw["daystosell"]/30,0) + 1
    
    return raw

def postalcode3 (raw):
    raw["postalcode3"] = raw["Postal Code"].str[:3]
    return raw

raw = changetype(raw)
raw = daystosell(raw)
raw = postalcode3(raw)
raw

"""## Data Exploration

!!!!!! Locker = nan / TotalParking = nan / stories in building = nan
"""

raw["Sold Price per TotFlArea"] = raw["Sold Price"]/(raw['TotFlArea']*0.0929)
#raw = raw[raw["Sold Price per TotFlArea"]<=60000]
raw.describe().round(0)

raw["daystosell_log"] = np.log(raw["daystosell"]+1)
plt.boxplot(raw["daystosell_log"])
plt.ylabel("Nb of Days to Sell")

raw["Sold Price per TotFlArea_log"] = np.log(raw["Sold Price per TotFlArea"])
plt.boxplot(raw["Sold Price per TotFlArea_log"])
plt.ylabel("Price per Square Meter")

raw["Sold Price_log"] = np.log(raw["Sold Price"])
plt.boxplot(raw["Sold Price_log"])
plt.ylabel("Sold Price_log")

plt.hist(raw["Sold Price"],bins=30)
plt.xlabel("Sold Price") 
plt.title("Histogram of Selling Price - Vancouver")

plt.hist(raw["Sold Price_log"],bins=50)
plt.xlabel("Sold Price (log)") 
plt.title("Histogram of Selling Price - Vancouver")

"""!!!! Yr Blt contains 9999

Now we want to plot daystosell as an histogram We also want to plot daystosell in function of difference between list price and sold price
"""

plt.hist(raw["daystosell"],bins=30)
plt.xlabel("Days for a house to be sold")
plt.title("Histogram of Days to Sell a house - Vancouver")

plt.hist(raw["daystosell_log"])
plt.xlabel("Days for a house to be sold (log)")
plt.title("Histogram of Days to Sell a house - Vancouver")

def listsold_diff(raw):
    raw["listsold_diff"] = raw["Sold Price"] - raw["List Price"]
    raw["listsold_diff_norm"] = raw["listsold_diff"] / raw["List Price"]
    return raw

raw = listsold_diff(raw)

plt.hist(raw["listsold_diff_norm"])
plt.xlabel("Difference between List and Sold Price (normalised)")

plt.scatter(raw["daystosell"],raw["listsold_diff_norm"])
plt.xlabel("Days for a house to be sold") 
plt.ylabel("Sold Price minus List (normalised)") 
plt.title("Link Between Price (over/under) estimation & Time to Sell")

"""## Latitude Longitude

### Import Data

Import Zipcode LatLon
"""

Zip_latlon = pd.read_excel("/content/drive/My Drive/Received Data/Zipcode LatLon.xlsx")
del Zip_latlon["provider"]
Zip_latlon.columns = ["Postal Code", "latitude", "longitude"]

Zip_latlon['longitude'] = np.where(Zip_latlon['latitude'] > 49.6, Zip_latlon["longitude"].mean(), Zip_latlon['longitude']).tolist()
Zip_latlon['latitude'] = np.where(Zip_latlon['latitude'] > 49.6, Zip_latlon["latitude"].mean(), Zip_latlon['latitude']).tolist()
Zip_latlon

"""Import Park"""

park_final = pd.read_excel("/content/drive/My Drive/Received Data/parks_final.xls")
park_final.columns = ["ID", "park", "hectare","lat_p","lon_p"]

import plotly.express as px

fig = px.scatter_mapbox(park_final, lat="lat_p", lon="lon_p",
                        color_discrete_sequence=["fuchsia"])
fig.update_layout(mapbox_style="open-street-map")
fig.update_layout(margin={"r":0,"t":0,"l":0,"b":0})
fig.show()

"""Import Cultural space"""

cultural_spaces_final = pd.read_excel("/content/drive/My Drive/Received Data/cultural-spaces_final.xls")
cultural_spaces_final.columns = ["CULTURAL_SPACE_NAME","lon_cs","lat_cs"]
cultural_spaces_final

import plotly.express as px

fig = px.scatter_mapbox(cultural_spaces_final, lat="lat_cs", lon="lon_cs",
                        color_discrete_sequence=["fuchsia"])
fig.update_layout(mapbox_style="open-street-map")
fig.update_layout(margin={"r":0,"t":0,"l":0,"b":0})
fig.show()

"""Import Libraries"""

libraries_final = pd.read_excel("/content/drive/My Drive/Received Data/libraries_final.xls")
libraries_final.columns = ["Lib_NAME","LocArea","lon_l","lat_l"]
libraries_final

import plotly.express as px

fig = px.scatter_mapbox(libraries_final, lat="lat_l", lon="lon_l",
                        color_discrete_sequence=["fuchsia"])
fig.update_layout(mapbox_style="open-street-map")
fig.update_layout(margin={"r":0,"t":0,"l":0,"b":0})
fig.show()

"""### Park distance from Zipcode"""

Zip_latlon['tmp'] = 1
park_final['tmp'] = 1

ZipPark = pd.merge(Zip_latlon, park_final, on=['tmp'])
ZipPark = ZipPark.drop('tmp', axis=1)

ZipPark

import math

def get_distance(lat_1, lng_1, lat_2, lng_2): 
    d_lat = lat_2 - lat_1
    d_lng = lng_2 - lng_1 

    temp = (  
         math.sin(d_lat / 2) ** 2 
       + math.cos(lat_1) 
       * math.cos(lat_2) 
       * math.sin(d_lng / 2) ** 2
    )

    return 6373.0 * (2 * math.atan2(math.sqrt(temp), math.sqrt(1 - temp)))

print (len(ZipPark))

ZipPark_dist = list()
for i in range(len(ZipPark)) :
  distance = get_distance(ZipPark.iloc[i,:]["latitude"], ZipPark.iloc[i,:]["longitude"], ZipPark.iloc[i,:]["lat_p"], ZipPark.iloc[i,:]["lon_p"])

  ZipPark_dist.append({"Postal Code": ZipPark.iloc[i,:]["Postal Code"], "park" : ZipPark.iloc[i,:]["park"],"hectar":ZipPark.iloc[i,:]["hectare"],"distance":distance})

ZipPark_dist = pd.DataFrame (ZipPark_dist)

ZipPark_dist

ZipPark_dist.to_csv('/content/drive/My Drive/BUs/ZipPark_dist.csv')

ZipPark_dist = pd.read_csv('/content/drive/My Drive/BUs/ZipPark_dist.csv', index_col=0)

ZipPark_dist_avg_min = ZipPark_dist.groupby("Postal Code").agg({"distance":["min","mean"]})
ZipPark_dist_avg_min.columns = ["min_park","mean_park"]
ZipPark_dist_avg_min

ZipPark_dist_inf100 = ZipPark_dist[ZipPark_dist["distance"]<=100]
ZipPark_dist_inf100_agg = ZipPark_dist_inf100.groupby("Postal Code").agg({"distance":["mean"],"hectar":["mean","max","count"]})
ZipPark_dist_inf100_agg.columns = ["mean_park_inf_100","mean_hectar_inf_100","max_hectar_inf_100","count_park_inf_100"]
ZipPark_dist_inf100_agg

"""### Cultural Space distance from Zipcode"""

Zip_latlon['tmp'] = 1
cultural_spaces_final['tmp'] = 1

ZipCS = pd.merge(Zip_latlon, cultural_spaces_final, on=['tmp'])
ZipCS = ZipCS.drop('tmp', axis=1)

ZipCS

import math

def get_distance(lat_1, lng_1, lat_2, lng_2): 
    d_lat = lat_2 - lat_1
    d_lng = lng_2 - lng_1 

    temp = (  
         math.sin(d_lat / 2) ** 2 
       + math.cos(lat_1) 
       * math.cos(lat_2) 
       * math.sin(d_lng / 2) ** 2
    )

    return 6373.0 * (2 * math.atan2(math.sqrt(temp), math.sqrt(1 - temp)))

ZipCS_dist = list()
for i in range(len(ZipCS)) :
  print(i)
  distance = get_distance(ZipCS.iloc[i,:]["latitude"], ZipCS.iloc[i,:]["longitude"], ZipCS.iloc[i,:]["lat_cs"], ZipCS.iloc[i,:]["lon_cs"])

  ZipCS_dist.append({"Postal Code": ZipCS.iloc[i,:]["Postal Code"], "cultural_space" : ZipCS.iloc[i,:]["CULTURAL_SPACE_NAME"],"distance":distance})

ZipCS_dist = pd.DataFrame (ZipCS_dist)

ZipCS_dist

ZipCS_dist.to_csv('/content/drive/My Drive/BUs/ZipCS_dist.csv')

ZipCS_dist = pd.read_csv('/content/drive/My Drive/BUs/ZipCS_dist.csv', index_col=0)

ZipCS_dist_avg_min = ZipCS_dist.groupby("Postal Code").agg({"distance":["min","mean"]})
ZipCS_dist_avg_min.columns = ["min_CulturalSpace","mean_CulturalSpace"]
ZipCS_dist_avg_min

ZipCS_dist_inf100 = ZipCS_dist[ZipCS_dist["distance"]<=100]
ZipCS_dist_inf100_agg = ZipCS_dist_inf100.groupby("Postal Code").agg({"distance":["mean","count"]})
ZipCS_dist_inf100_agg.columns = ["mean_Cultural_Space_inf_100","count_Cultural_Space_inf_100"]
ZipCS_dist_inf100_agg

"""### Libraries distance from Zipcode"""

Zip_latlon['tmp'] = 1
libraries_final['tmp'] = 1

ZipLib = pd.merge(Zip_latlon, libraries_final, on=['tmp'])
ZipLib = ZipLib.drop('tmp', axis=1)

ZipLib

ZipLib_dist = list()
for i in range(len(ZipLib)) :
  distance = get_distance(ZipLib.iloc[i,:]["latitude"], ZipLib.iloc[i,:]["longitude"], ZipLib.iloc[i,:]["lat_l"], ZipLib.iloc[i,:]["lon_l"])

  ZipLib_dist.append({"Postal Code": ZipLib.iloc[i,:]["Postal Code"], "libname" : ZipLib.iloc[i,:]["Lib_NAME"],"distance":distance})

ZipLib_dist = pd.DataFrame (ZipLib_dist)

ZipLib_dist

ZipLib_dist.to_csv('/content/drive/My Drive/BUs/ZipLib_dist.csv')

ZipLib_dist = pd.read_csv('/content/drive/My Drive/BUs/ZipLib_dist.csv', index_col=0)

ZipLib_dist_avg_min = ZipLib_dist.groupby("Postal Code").agg({"distance":["min","mean"]})
ZipLib_dist_avg_min.columns = ["min_Lib","mean_Lib"]
ZipLib_dist_avg_min

ZipLib_dist_inf100 = ZipLib_dist[ZipLib_dist["distance"]<=100]
ZipLib_dist_inf100_agg = ZipLib_dist_inf100.groupby("Postal Code").agg({"distance":["mean","count"]})
ZipLib_dist_inf100_agg.columns = ["mean_lib_inf_100","count_lib_inf_100"]
ZipLib_dist_inf100_agg

"""### Merge All Infos"""

final_zip = pd.merge(Zip_latlon, ZipPark_dist_avg_min, on ='Postal Code',how ='left')
final_zip = pd.merge(final_zip, ZipPark_dist_inf100_agg, on ='Postal Code',how ='left')

final_zip = pd.merge(final_zip, ZipCS_dist_avg_min, on ='Postal Code',how ='left')
final_zip = pd.merge(final_zip, ZipCS_dist_inf100_agg, on ='Postal Code',how ='left')

final_zip = pd.merge(final_zip, ZipLib_dist_avg_min, on ='Postal Code',how ='left')
final_zip = pd.merge(final_zip, ZipLib_dist_inf100_agg, on ='Postal Code',how ='left')

final_zip

final_zip.to_csv('/content/drive/My Drive/BUs/final_zip.csv')

final_zip = pd.read_csv('/content/drive/My Drive/BUs/final_zip.csv')

import plotly.express as px

fig = px.scatter_mapbox(final_zip, lat="latitude", lon="longitude",
                        color_discrete_sequence=["fuchsia"])
fig.update_layout(mapbox_style="open-street-map")
fig.update_layout(margin={"r":0,"t":0,"l":0,"b":0})
fig.show()

"""### Merge with Raw"""

raw = pd.merge(raw, final_zip, on ='Postal Code',how ='left')

#del raw["latitude"]
#del raw["longitude"]

raw.head()

map_average_price = raw.groupby(['latitude', 'longitude']).agg({'Sold Price per TotFlArea': ['mean'],'daystosell': ['mean']}).reset_index()
map_average_price.columns = ["latitude","longitude","Sold Price per TotFlArea_mean",'daystosell_mean']

import plotly.express as px

fig = px.scatter_mapbox(map_average_price, lat="latitude", lon="longitude", size= "Sold Price per TotFlArea_mean",
                        color_continuous_scale="Sold Price per TotFlArea_mean")
fig.update_layout(mapbox_style="open-street-map")
fig.update_layout(margin={"r":0,"t":0,"l":0,"b":0})
fig.show()

import plotly.express as px

fig = px.scatter_mapbox(map_average_price, lat="latitude", lon="longitude", size= "daystosell_mean",
                        color_continuous_scale="daystosell_mean")
fig.update_layout(mapbox_style="open-street-map")
fig.update_layout(margin={"r":0,"t":0,"l":0,"b":0})
fig.show()

"""## Manual Extraction of Features - by Law Restriction

Create bolean for By law Restrictions

*   AGER --> limitation of age
*   SMKG --> non smocking
"""

raw['AGER'] = raw['Bylaw Restrictions'].apply(lambda x : '1' if 'AGER' in x else '0').astype(int)
raw['PETN'] = raw['Bylaw Restrictions'].apply(lambda x : '1' if 'PETN' in x else '0').astype(int)
raw['PETR'] = raw['Bylaw Restrictions'].apply(lambda x : '1' if 'PETR' in x else '0').astype(int)
raw['PETY'] = raw['Bylaw Restrictions'].apply(lambda x : '1' if 'PETY' in x else '0').astype(int)
raw['RENN'] = raw['Bylaw Restrictions'].apply(lambda x : '1' if 'RENN' in x else '0').astype(int)
raw['RENY'] = raw['Bylaw Restrictions'].apply(lambda x : '1' if 'RENY' in x else '0').astype(int)
raw['RENR'] = raw['Bylaw Restrictions'].apply(lambda x : '1' if 'RENR' in x else '0').astype(int)
raw['SMKG'] = raw['Bylaw Restrictions'].apply(lambda x : '1' if 'SMKG' in x else '0').astype(int)
raw['NO'] = raw['Bylaw Restrictions'].apply(lambda x : '1' if 'NO' in x else '0').astype(int)
raw['nan'] = raw['Bylaw Restrictions'].apply(lambda x : '1' if 'nan' in x else '0').astype(int)

"""Make Parking with no NaN"""

raw['TotalPrkng_2'] = raw['TotalPrkng'].replace("nan", 0).astype(float)

"""Make Stories in building with no NaN"""

raw['Stories in Building_2'] = raw['Stories in Building'].replace("nan", 0).astype(float)

"""## Bag of Word for View

### Extract Words
"""

from sklearn.feature_extraction.text import CountVectorizer
vect = CountVectorizer()

vect.fit(raw['View'])

view_bag_of_words = pd.DataFrame(vect.transform(raw['View']).todense(), columns = vect.get_feature_names())
view_bag_of_words

"""### Remove numbers & stop words"""

remove_number_list = ["0","1","2","3","4","5","6","7","8","9"]

for i in remove_number_list:
  view_bag_of_words = view_bag_of_words [view_bag_of_words.columns.drop(list(view_bag_of_words.filter(regex=i)))]

view_bag_of_words.head()

words_weight = list()
for i in range(len(view_bag_of_words.columns)):
  view_bag_of_words["view"].sum()
  words_weight.append({"word_name" : view_bag_of_words.columns[i],"word_weight":view_bag_of_words[view_bag_of_words.columns[i]].sum()})

remove_word_list = ["and","of","to","false","some","from","the","over","you","your","would","www","with","work","will","are","at","be",
                    "by","com","for","if","in","is","no","on","or","up","via","very","as","am","an","any","also","all","well","amd",
                    "around","ba","bah","bak","yes","yt","yup"]

words_weight = pd.DataFrame(words_weight)
for i in remove_word_list :
  words_weight = words_weight[words_weight["word_name"]!=i]

#words_weight = words_weight[words_weight["word_weight"]>100]
print(words_weight)

view_bag_of_words_final = view_bag_of_words[words_weight["word_name"]]

# adjust mountains
view_bag_of_words_final["mountain_fi"] = view_bag_of_words_final["mountain"] + view_bag_of_words_final["mountains"] + view_bag_of_words_final["mtn"] + view_bag_of_words_final["mtns"]
del view_bag_of_words_final["mountain"]
del view_bag_of_words_final["mountains"]
del view_bag_of_words_final["mtn"] 
del view_bag_of_words_final["mtns"]

# adjust views
view_bag_of_words_final["view_fi"] = view_bag_of_words_final["view"] + view_bag_of_words_final["views"]
del view_bag_of_words_final["view"]
del view_bag_of_words_final["views"]

view_bag_of_words_final.head()

"""### Lemme Word"""

# Importing Porterstemmer from nltk library
# Checking for the word ‘giving’ 
from nltk.stem import PorterStemmer
pst = PorterStemmer()

categories = list()
for i in range(len(view_bag_of_words_final.columns)):
  categories.append({"name":view_bag_of_words_final.columns[i],"newname":pst.stem(view_bag_of_words_final.columns[i])})
categories = pd.DataFrame(categories)
print(categories)

view_bag_of_words_final_T = view_bag_of_words_final.T
view_bag_of_words_final_T = pd.concat([view_bag_of_words_final_T.reset_index(drop=True), categories], axis=1)
view_bag_of_words_final_grouped = view_bag_of_words_final_T.groupby(view_bag_of_words_final_T["newname"]).sum().T
view_bag_of_words_final_grouped

"""### reduce dimension PCA"""

from sklearn.decomposition import PCA

pca = PCA(n_components = 13)
pca.fit(view_bag_of_words_final)

pca.explained_variance_ratio_

plt.bar(["1","2","3","4","5","6","7","8","9","10","11","12","13"],pca.explained_variance_ratio_)
plt.xlabel("Nb of Loads") 
plt.ylabel("Explained Variance Ratio") 
plt.title("Principal Component Analysis - View")

pca_view_bag_of_words = pca.transform(view_bag_of_words_final)
pca_view_bag_of_words = pd.DataFrame(pca_view_bag_of_words)
pca_view_bag_of_words = pca_view_bag_of_words.add_prefix("pca_view_")
pca_view_bag_of_words

loadings = pca.components_.T * np.sqrt(pca.explained_variance_)
loading_matrix = pd.DataFrame(loadings, index=view_bag_of_words_final.columns).round(2)

threshold = 0.05
PCA_features = list()

for c in range(len(loading_matrix.columns)):
  for i in range(len(loading_matrix)):
    if abs(loading_matrix.iloc[i,c]) >= threshold:
      PCA_features.append({"PCA_name":loading_matrix.columns[c], "feature_name":view_bag_of_words_final.columns[i],
                         "explained_variance":loading_matrix.iloc[i,c]})
    
pd.DataFrame(PCA_features).head(50)

"""### Clustering"""

from sklearn import mixture

cl_train, cl_valid, _, _ = train_test_split(pca_view_bag_of_words, pca_view_bag_of_words, test_size=0.2, random_state=0)
loss_train, loss_valid = [], []

max_cluster = [30,40,50,60,70,80]

for k in max_cluster:
    GMM = mixture.GaussianMixture(n_components=k, covariance_type='full')
    GMM.fit(cl_train)
    
    loss_train.append(GMM.score(cl_train))
    loss_valid.append(GMM.score(cl_valid))
            
plt.plot(max_cluster, loss_train, label='Train')
plt.plot(max_cluster, loss_valid, label='Validation')

plt.xlabel('Number of clusters')
plt.ylabel('Log-Likelihood (Maximize)')
plt.title('Clustering Analysis - View PCA')

# Ghosting the legend
leg = plt.gca().legend(loc='center left', bbox_to_anchor=(1, .85))
leg.get_frame().set_alpha(0)

GMM = mixture.GaussianMixture(n_components=50, covariance_type='full')
GMM_label = pd.DataFrame(GMM.fit_predict(pca_view_bag_of_words))
GMM_label.columns = ["cluster_view"]
pca_view_bag_of_words_cl = pd.concat([GMM_label.reset_index(drop=True), pca_view_bag_of_words], axis=1)
pca_view_bag_of_words_cl

pca_view_bag_of_words_cl.to_csv('/content/drive/My Drive/BUs/pca_view_bag_of_words_cl.csv')

pca_view_bag_of_words_cl = pd.read_csv('/content/drive/My Drive/BUs/pca_view_bag_of_words_cl.csv', index_col=0)

"""## Bag of Words for Public Remark

### Extract Word
"""

vect.fit(raw['Public Remarks'])

public_remarks_bag_of_words = pd.DataFrame(vect.transform(raw['Public Remarks']).todense(), columns = vect.get_feature_names())
public_remarks_bag_of_words

"""### Remove numbers and stop words"""

remove_number_list = ["0","1","2","3","4","5","6","7","8","9","_","aa","œ"]
for i in remove_number_list:
  public_remarks_bag_of_words = public_remarks_bag_of_words[public_remarks_bag_of_words.columns.drop(list(public_remarks_bag_of_words.filter(regex=i)))]

public_remarks_bag_of_words.head()

public_remarks_words_weight = list()
for i in range(len(public_remarks_bag_of_words.columns)):
  public_remarks_words_weight.append({"word_name" : public_remarks_bag_of_words.columns[i],"word_weight":public_remarks_bag_of_words[public_remarks_bag_of_words.columns[i]].sum()})

public_remarks_words_weight = pd.DataFrame(public_remarks_words_weight)
public_remarks_words_weight = public_remarks_words_weight[public_remarks_words_weight["word_weight"]>5]

remove_word_list = ["and","of","to","false","some","from","the","over","you","your","would","www","with","work","will","are","at","be",
                    "by","com","for","if","in","is","no","on","or","up","via","very","as","am","an","any","also","all","well","amd",
                    "around","ba","bah","bak","yes","yt","yup","ab","about","wu","wv","strata","http"]

for i in remove_word_list :
  public_remarks_words_weight = public_remarks_words_weight[public_remarks_words_weight["word_name"]!=i]

print(public_remarks_words_weight)

public_remarks_bag_of_words_final = public_remarks_bag_of_words[public_remarks_words_weight["word_name"]]
public_remarks_bag_of_words_final.head()

"""### Lemme Word"""

# Importing Porterstemmer from nltk library
# Checking for the word ‘giving’ 
from nltk.stem import PorterStemmer
pst = PorterStemmer()

categories = list()
for i in range(len(public_remarks_bag_of_words_final.columns)):
  categories.append({"name":public_remarks_bag_of_words_final.columns[i],"newname":pst.stem(public_remarks_bag_of_words_final.columns[i])})
  #print(public_remarks_bag_of_words_final.columns[i])
  #print(pst.stem(public_remarks_bag_of_words_final.columns[i]))
categories = pd.DataFrame(categories)
print(categories)

public_remarks_bag_of_words_final_T = public_remarks_bag_of_words_final.T
public_remarks_bag_of_words_final_T = pd.concat([public_remarks_bag_of_words_final_T.reset_index(drop=True), categories], axis=1)
public_remarks_bag_of_words_final_grouped = public_remarks_bag_of_words_final_T.groupby(public_remarks_bag_of_words_final_T["newname"]).sum().T
public_remarks_bag_of_words_final_grouped

"""### Inverse Frequency"""

#public_remarks_bag_of_words_final_grouped = public_remarks_bag_of_words_final_grouped.divide(public_remarks_bag_of_words_final_grouped.sum().sum())

"""### Reduce Dimensions - PCA"""

from sklearn.decomposition import PCA

pca = PCA(n_components = 10)
pca.fit(public_remarks_bag_of_words_final_grouped)

pca.explained_variance_ratio_

plt.bar(["1","2","3","4","5","6","7","8","9","10"],pca.explained_variance_ratio_)
plt.xlabel("Nb of Loads") 
plt.ylabel("Explained Variance Ratio") 
plt.title("Principal Component Analysis - Public Remark")

pca_public_remarks_bag_of_words = pca.transform(public_remarks_bag_of_words_final_grouped)
pca_public_remarks_bag_of_words = pd.DataFrame(pca_public_remarks_bag_of_words)
pca_public_remarks_bag_of_words = pca_public_remarks_bag_of_words.add_prefix("pca_public_remarks_")
pca_public_remarks_bag_of_words

loadings = pca.components_.T * np.sqrt(pca.explained_variance_)
loading_matrix = pd.DataFrame(loadings, index=public_remarks_bag_of_words_final_grouped.columns).round(2)
threshold = 0.1
PCA_features = list()

for c in range(len(loading_matrix.columns)):
  for i in range(len(loading_matrix)):
    if abs(loading_matrix.iloc[i,c]) >= threshold:
      PCA_features.append({"PCA_name":loading_matrix.columns[c], "feature_name":public_remarks_bag_of_words_final_grouped.columns[i],
                         "explained_variance":loading_matrix.iloc[i,c]})

pd.DataFrame(PCA_features).tail(25)

"""### clustering"""

from sklearn import mixture

cl_train, cl_valid, _, _ = train_test_split(pca_public_remarks_bag_of_words, pca_public_remarks_bag_of_words, test_size=0.2, random_state=0)
loss_train, loss_valid = [], []

max_cluster = [50,75,100,125,150,175]

for k in max_cluster:
    GMM = mixture.GaussianMixture(n_components=k, covariance_type='full')
    GMM.fit(cl_train)
    
    loss_train.append(GMM.score(cl_train))
    loss_valid.append(GMM.score(cl_valid))
            
plt.plot(max_cluster, loss_train, label='Train')
plt.plot(max_cluster, loss_valid, label='Validation')

plt.xlabel('Number of clusters')
plt.ylabel('Log-Likelihood (Maximize)')
plt.title('Clustering Analysis - Public Remark PCA')

# Ghosting the legend
leg = plt.gca().legend(loc='center left', bbox_to_anchor=(1, .85))
leg.get_frame().set_alpha(0)

GMM = mixture.GaussianMixture(n_components=150, covariance_type='full')
GMM_label = pd.DataFrame(GMM.fit_predict(pca_public_remarks_bag_of_words))
GMM_label.columns = ["cluster_public_remarks"]
pca_public_remarks_bag_of_words_cl = pd.concat([GMM_label.reset_index(drop=True), pca_public_remarks_bag_of_words], axis=1)
pca_public_remarks_bag_of_words_cl

pca_public_remarks_bag_of_words_cl.to_csv('/content/drive/My Drive/BUs/pca_public_remarks_bag_of_words_cl.csv')

pca_public_remarks_bag_of_words_cl = pd.read_csv('/content/drive/My Drive/BUs/pca_public_remarks_bag_of_words_cl.csv', index_col=0)

"""# Feature Ingeneering - Sold Price

## Get final X & Y
"""

def X_Y_SP (raw,pca_view_bag_of_words_cl,pca_public_remarks_bag_of_words_cl):
    
    X_SP = raw.loc[:,["PicCount","List Price","day_month_year","TotFlArea",#"listsold_diff_norm","List Price"
               'AGER','PETN','PETR','PETY','RENN','RENY','RENR','SMKG','NO','nan',
               "Tot BR","Tot Baths","Yr Blt","Age","Locker",'TotalPrkng_2',"S/A","TypeDwel",#"Floor Area Fin - Total",
                "Foundation","Full Baths","Half Baths","No. Floor Levels",
                "Stories in Building_2","Zoning","postalcode3"]]#"postalcode3"
    X_SP = pd.concat([X_SP.reset_index(drop=True), pca_view_bag_of_words_cl], axis=1)
    X_SP = pd.concat([X_SP.reset_index(drop=True), pca_public_remarks_bag_of_words_cl], axis=1)

    X_SP = pd.get_dummies(X_SP, columns=['Locker','S/A','TypeDwel','Foundation','Zoning',"postalcode3","cluster_view","cluster_public_remarks"])#'postalcode3',"cluster_view","cluster_public_remarks"
    
    X_SP = X_SP.astype(float)
    
    Y_SP = raw.loc[:,"Sold Price"]
    
    return (X_SP,Y_SP)

X_SP,Y_SP = X_Y_SP (raw,pca_view_bag_of_words_cl,pca_public_remarks_bag_of_words_cl)

#X_SP.columns
# Index(['Unnamed: 0', 'a', 'b', 'c'], dtype='object')

#X_SP.columns.str.match('Unnamed') #Unnamed: 0 	Unnamed: 0.1
# array([ True, False, False, False])

X_SP = X_SP.loc[:, ~X_SP.columns.str.match('Unnamed')]

Y_SP

X_SP.to_csv('/content/drive/My Drive/BUs/X_SP.csv')

Y_SP.to_csv('/content/drive/My Drive/BUs/Y_SP.csv')

"""## Feature Expension"""

feature_to_expand = ["PicCount","day_month_year","TotFlArea","Tot BR",
                     "Tot Baths","Yr Blt","Age","TotalPrkng_2","Full Baths","Half Baths",
                     "No. Floor Levels","Stories in Building_2","List Price"] #"List Price"

X_featured_SP = X_SP.copy()
for i in feature_to_expand: #before having too many due to zipcode or view
  for j in feature_to_expand: #before having too many due to zipcode or view
    if j >= i:
      l = "exp_" + i + " " + j
      X_featured_SP[l+"_mlp"]= X_featured_SP[i] * X_featured_SP[j]
      if j!=i:
        X_featured_SP[l+"_min"]= X_featured_SP[i] - X_featured_SP[j]
    
X_featured_SP.head()

"""## Correlation"""

X_featured_SP.corr()

# Threshold for removing correlated variables
threshold = 0.9

# Absolute value correlation matrix
corr_matrix = X_featured_SP.corr().abs()

# Upper triangle of correlations
upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))

# Select columns with correlations above threshold
to_drop = [column for column in upper.columns if any(upper[column] > threshold)]

# Remove the columns
X_featured_SP = X_featured_SP.drop(columns = to_drop)

X_featured_SP.head()

X_featured_SP.to_csv('/content/drive/My Drive/BUs/X_featured_SP.csv')

X_featured_SP = pd.read_csv('/content/drive/My Drive/BUs/X_featured_SP.csv', index_col=0)

"""## Light GBM - Features Reduction"""

import lightgbm as lgb

def identify_zero_importance_features(train, train_labels, iterations = 2):
    
    # Initialize an empty array to hold feature importances
    feature_importances = np.zeros(train.shape[1])

    # Create the model with several hyperparameters
    model = lgb.LGBMClassifier(objective='regression', boosting_type = 'goss', 
                               n_estimators = 100, class_weight = 'balanced')
    
    # Fit the model multiple times to avoid overfitting
    for i in range(iterations):

        # Split into training and validation set
        train_features, valid_features, train_y, valid_y = train_test_split(train, train_labels, 
                                                                            test_size = 0.25, 
                                                                            random_state = i)

        # Train using early stopping
        model.fit(train_features, train_y)

        # Record the feature importances
        feature_importances += model.feature_importances_ / iterations
    
    feature_importances = pd.DataFrame({'feature': list(train.columns), 
                            'importance': feature_importances}).sort_values('importance', 
                                                                            ascending = False)
    
    # Find the features with zero importance
    zero_features = list(feature_importances[feature_importances['importance'] == 0.0]['feature'])
    print('\nThere are %d features with 0.0 importance' % len(zero_features))
    
    return zero_features, feature_importances

Y_labels_SP = np.array(Y_SP)
 zero_features_SP, feature_importances_SP = identify_zero_importance_features(X_featured_SP,  Y_labels_SP, iterations = 2)

feature_importances_SP.to_csv('/content/drive/My Drive/BUs/feature_importances_SP.csv')

feature_importances_SP = pd.read_csv('/content/drive/My Drive/BUs/feature_importances_SP.csv', index_col=0)

feature_importances_SP["cumsum"]=np.cumsum(feature_importances_SP["importance"])/feature_importances_SP["importance"].sum()
most_important_features_SP=feature_importances_SP[feature_importances_SP["cumsum"]<=0.98]
most_important_features_SP.head(10)

plt.bar(most_important_features_SP["feature"],most_important_features_SP["importance"])
plt.xlabel("Features")
plt.ylabel("Feature importance")
plt.title("Feautre importance - Price Prediction")

most_important_features_SP

X_final_SP = X_featured_SP.loc[:,most_important_features_SP.iloc[:,0]]
X_final_SP.head()

X_final_SP.to_csv('/content/drive/My Drive/BUs/X_final_SP.csv')

X_final_SP = pd.read_csv('/content/drive/My Drive/BUs/X_final_SP.csv', index_col=0)

"""# First Exploration - Sold Price

## First Model Linear Regression
"""

# Split Train/Test
# Keep 20% of the data for testing.
X_train_SP, X_test_SP, Y_train_SP, Y_test_SP = train_test_split(
    X_final_SP, Y_SP, test_size=0.2, random_state=1234, shuffle=True)

# Commented out IPython magic to ensure Python compatibility.
from sklearn import datasets, linear_model
from sklearn.metrics import mean_squared_error, r2_score

# Create linear regression object
reg_SP = linear_model.LinearRegression()
print(reg_SP)

# Train the model using the training sets
reg_SP.fit(X_train_SP, Y_train_SP)

# Make train predictions
Y_train_pred_SP = reg_SP.predict(X_train_SP)

print("Train Root Mean squared error: %.3f"
#       % mean_squared_error(Y_train_SP, Y_train_pred_SP)**0.5)

# Make test predictions
Y_test_pred_SP = reg_SP.predict(X_test_SP)

print("Test Root Mean squared error: %.3f"
#      % mean_squared_error(Y_test_SP, Y_test_pred_SP)**0.5)

Y_test_pred_SP = pd.DataFrame(Y_test_pred_SP)
Y_comb_test_SP = pd.concat([Y_test_SP.reset_index(), Y_test_pred_SP], axis=1, ignore_index=False)
Y_comb_test_SP.rename({"Sold Price": "Sold Price",  
           0: "Sold Price_pred"},  
          axis = "columns", inplace = True) 

Y_comb_test_SP.head()

plt.scatter(Y_comb_test_SP["Sold Price"],Y_comb_test_SP["Sold Price_pred"])
plt.xlabel("Sold Price") 
plt.ylabel("Predicted - Sold Price")
plt.title ("Predicted vs Sold Price - Linear Regression")

"""## Second Model - XGBoost reg:linear"""

# Commented out IPython magic to ensure Python compatibility.
import xgboost as xgb

# Create linear regression objec
xgbr_SP = xgb.XGBRegressor(verbosity=0)
print(xgbr_SP)

# Train the model using the training sets
xgbr_SP.fit(X_train_SP, Y_train_SP)


# Make train predictions
Y_train_pred_SP = xgbr_SP.predict(X_train_SP)

print("Train Root Mean squared error: %.3f"
#       % mean_squared_error(Y_train_SP, Y_train_pred_SP)**0.5)

# Make test predictions
Y_test_pred_SP = xgbr_SP.predict(X_test_SP)

print("Test Root Mean squared error: %.3f"
#      % mean_squared_error(Y_test_SP, Y_test_pred_SP)**0.5)

Y_test_pred_SP = pd.DataFrame(Y_test_pred_SP)
Y_comb_test_SP = pd.concat([Y_test_SP.reset_index(), Y_test_pred_SP], axis=1, ignore_index=False)
Y_comb_test_SP.rename({"Sold Price": "Sold Price",  
           0: "Sold Price_pred"},  
          axis = "columns", inplace = True) 

plt.scatter(Y_comb_test_SP["Sold Price"],Y_comb_test_SP["Sold Price_pred"])
plt.xlabel("Sold Price") 
plt.ylabel("Predicted - Sold Price")
plt.title ("Predicted vs Sold Price - XGBoost")

"""# Hyper Parameter Optimization - Sold Price

## Feature importance Selection
"""

importance_scalar_SP = [0.8,0.85,0.9,0.95,0.96,0.97,0.98,0.99]

def feature_selection_SP (importance_scalar_SP,feature_importances_SP,X_featured_SP, Y_SP):

  pred_dict_SP = list()

  for i in importance_scalar_SP:
    # selection of features
    most_important_features_SP=feature_importances_SP[feature_importances_SP["cumsum"]<=i]
    X_final_SP = X_featured_SP.loc[:,most_important_features_SP.iloc[:,1]]



    # train & test split
    X_train_SP, X_test_SP, Y_train_SP, Y_test_SP = train_test_split(
        X_final_SP, Y_SP, test_size=0.2, random_state=1234, shuffle=True)
    
    # Create linear regression objec
    xgbr_SP = xgb.XGBRegressor(verbosity=0)

    # Train the model using the training sets
    xgbr_SP.fit(X_train_SP, Y_train_SP)

    # Make test predictions
    Y_test_pred_SP = xgbr_SP.predict(X_test_SP)

    pred_dict_SP.append({"Importance_Param":i,"Y_test_error": mean_squared_error(Y_test_SP, Y_test_pred_SP)**0.5})

  return (pred_dict_SP)

pred_dict_importance_param_SP = feature_selection_SP (importance_scalar_SP,feature_importances_SP,X_featured_SP, Y_SP)

print(pd.DataFrame(pred_dict_importance_param_SP))

"""## Feature Type"""

feature_to_remove_SP = ["List Price","exp","cluster_public_remarks","pca_public_remarks","pca_view","cluster_view","postalcode3"]

def feature_impact (feature_to_remove_SP,X_train_SP,Y_train_SP,X_test_SP,Y_test_SP):

  pred_dict_SP = list()

  for i in feature_to_remove_SP:    
    # Create linear regression objec
    xgbr_SP = xgb.XGBRegressor(verbosity=0)

    # features to remove
    X_train_SP_bis = X_train_SP [X_train_SP.columns.drop(list(X_train_SP.filter(regex=i)))]
    X_test_SP_bis = X_test_SP [X_test_SP.columns.drop(list(X_test_SP.filter(regex=i)))]

    # Train the model using the training sets
    xgbr_SP.fit(X_train_SP_bis, Y_train_SP)

    # Make test predictions
    Y_test_pred_SP = xgbr_SP.predict(X_test_SP_bis)

    pred_dict_SP.append({"Removed Feature":i,"Y_test_error": mean_squared_error(Y_test_SP, Y_test_pred_SP)**0.5})

  return (pred_dict_SP)

pred_dict_feature_to_remove_SP = feature_impact (feature_to_remove_SP,X_train_SP,Y_train_SP,X_test_SP,Y_test_SP)

print(pd.DataFrame(pred_dict_feature_to_remove_SP))

"""### Test"""

feature_to_remove_SP_test = ["exp","cluster_public_remarks","pca_public_remarks","pca_view","cluster_view","postalcode3"]

feature_to_remove_SP_test = ["exp","cluster_public_remarks","pca_public_remarks","pca_view","cluster_view"]

X_train_SP_bis = X_train_SP.copy()
X_test_SP_bis = X_test_SP.copy()

for i in feature_to_remove_SP_test:    
  # Create linear regression objec
  xgbr_SP = xgb.XGBRegressor(verbosity=0)

  # features to remove
  X_train_SP_bis = X_train_SP_bis [X_train_SP_bis.columns.drop(list(X_train_SP_bis.filter(regex=i)))]
  X_test_SP_bis = X_test_SP_bis [X_test_SP_bis.columns.drop(list(X_test_SP_bis.filter(regex=i)))]

  # Train the model using the training sets
xgbr_SP.fit(X_train_SP_bis, Y_train_SP)

  # Make test predictions
Y_test_pred_SP = xgbr_SP.predict(X_test_SP_bis)

print(mean_squared_error(Y_test_SP, Y_test_pred_SP)**0.5)

X_train_SP_bis



"""## Learning Rate Selection"""

learning_rate_SP = [0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4]

def learning_rate_selection_SP (learning_rate_SP,X_train_SP,Y_train_SP,X_test_SP,Y_test_SP):

  pred_dict_SP = list()

  for i in learning_rate_SP:    
    # Create linear regression objec
    xgbr_SP = xgb.XGBRegressor(verbosity=0, learning_rate=i)


    # Train the model using the training sets
    xgbr_SP.fit(X_train_SP, Y_train_SP)

    # Make test predictions
    Y_test_pred_SP = xgbr_SP.predict(X_test_SP)

    pred_dict_SP.append({"Eta_Param":i,"Y_test_error": mean_squared_error(Y_test_SP, Y_test_pred_SP)**0.5})

  return (pred_dict_SP)

pred_dict_learning_rate_SP = learning_rate_selection_SP (learning_rate_SP,X_train_SP,Y_train_SP,X_test_SP,Y_test_SP)

print(pd.DataFrame(pred_dict_learning_rate_SP))

"""## Max Depth Selection"""

max_depth_SP = [1,2,3,4,5,6,7,8,9,10]

def max_depth_selection_SP (max_depth_SP,X_train_SP,Y_train_SP,X_test_SP,Y_test_SP):

  pred_dict_SP = list()

  for i in max_depth_SP:    
    # Create linear regression objec
    xgbr_SP = xgb.XGBRegressor(verbosity=0, learning_rate=0.15, max_depth = i)

    # Train the model using the training sets
    xgbr_SP.fit(X_train_SP, Y_train_SP)

    # Make test predictions
    Y_test_pred_SP = xgbr_SP.predict(X_test_SP)

    pred_dict_SP.append({"max_depth":i,"Y_test_error": mean_squared_error(Y_test_SP, Y_test_pred_SP)**0.5})

  return (pred_dict_SP)

pred_dict_max_depth_SP = max_depth_selection_SP (max_depth_SP,X_train_SP,Y_train_SP,X_test_SP,Y_test_SP)

print(pd.DataFrame(pred_dict_max_depth_SP))

"""## N rounds Selection"""

n_estimators_SP = [100,110,120,130,140,150,160,170,180,190]

def n_estimators_selection_SP (n_estimators_SP,X_train_SP,Y_train_SP,X_test_SP,Y_test_SP):

  pred_dict_SP = list()

  for i in n_estimators_SP:    
    # Create linear regression objec
    xgbr_SP = xgb.XGBRegressor(verbosity=0, learning_rate=0.15, max_depth = 6, n_estimators=i)

    # Train the model using the training sets
    xgbr_SP.fit(X_train_SP, Y_train_SP)

    # Make test predictions
    Y_test_pred_SP = xgbr_SP.predict(X_test_SP)

    pred_dict_SP.append({"n_estimators":i,"Y_test_error": mean_squared_error(Y_test_SP, Y_test_pred_SP)**0.5})

  return (pred_dict_SP)

pred_dict_n_estimators_SP = n_estimators_selection_SP (n_estimators_SP,X_train_SP,Y_train_SP,X_test_SP,Y_test_SP)

print(pd.DataFrame(pred_dict_n_estimators_SP))

"""# Feature Ingeneering - Days to Sell

## Get final X & Y

**Get final X & Y**

Excluding "Lot Sz (Sq.Ft.)" & "StratMtFee" due to error in the Logistic Regression

!! DOM is an indicator for the time to sell

Excluding Floor Area - too corrolated to TotFlArea
"""

def X_Y (raw,pca_view_bag_of_words_cl,pca_public_remarks_bag_of_words_cl):
    
    X = raw.loc[:,["PicCount","Sold Price","List Price","day_month_year","TotFlArea",#"listsold_diff_norm",
               'AGER','PETN','PETR','PETY','RENN','RENY','RENR','SMKG','NO','nan',
               "Tot BR","Tot Baths","Yr Blt","Age","Locker",'TotalPrkng_2',"S/A","TypeDwel",#"Floor Area Fin - Total",
                "Foundation","Full Baths","Half Baths","No. Floor Levels",
                "Stories in Building_2","Zoning","postalcode3"]]#"postalcode3"
    X = pd.concat([X.reset_index(drop=True), pca_view_bag_of_words_cl], axis=1)
    X = pd.concat([X.reset_index(drop=True), pca_public_remarks_bag_of_words_cl], axis=1)

    X = pd.get_dummies(X, columns=['Locker','S/A','TypeDwel','Foundation','Zoning',"postalcode3"])#'postalcode3',"cluster_view","cluster_public_remarks"
    
    X = X.astype(float)
    
    Y = raw.loc[:,"daystosell_log"]
    
    return (X,Y)

X,Y = X_Y (raw,pca_view_bag_of_words_cl,pca_public_remarks_bag_of_words_cl)

X.head()

Y.head()

"""## Features Expension"""

X_featured = X.copy()
for i in range (0,15): #before having too many due to zipcode or view
  for j in range (0,15): #before having too many due to zipcode or view
    if j >= i:
      l = "exp_" + X.columns[i] + " " + X.columns[j]
      X_featured[l+"_mlp"]= X_featured[X.columns[i]] * X_featured[X.columns[j]]
      if j!=i:
        X_featured[l+"_min"]= X_featured[X.columns[i]] - X_featured[X.columns[j]]
    
X_featured.head()

"""## Correlation"""

X_featured.corr()

# Threshold for removing correlated variables
threshold = 0.9

# Absolute value correlation matrix
corr_matrix = X_featured.corr().abs()

# Upper triangle of correlations
upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))

# Select columns with correlations above threshold
to_drop = [column for column in upper.columns if any(upper[column] > threshold)]

# Remove the columns
X_featured = X_featured.drop(columns = to_drop)

X_featured.head()

"""## Light GBM - Features Reduction

!!! need to do with train data !!!
"""

import lightgbm as lgb

def identify_zero_importance_features(train, train_labels, iterations = 2):
    
    # Initialize an empty array to hold feature importances
    feature_importances = np.zeros(train.shape[1])

    # Create the model with several hyperparameters
    model = lgb.LGBMClassifier(objective='regression', boosting_type = 'goss', 
                               n_estimators = 100, class_weight = 'balanced')
    
    # Fit the model multiple times to avoid overfitting
    for i in range(iterations):

        # Split into training and validation set
        train_features, valid_features, train_y, valid_y = train_test_split(train, train_labels, 
                                                                            test_size = 0.25, 
                                                                            random_state = i)

        # Train using early stopping
        model.fit(train_features, train_y)

        # Record the feature importances
        feature_importances += model.feature_importances_ / iterations
    
    feature_importances = pd.DataFrame({'feature': list(train.columns), 
                            'importance': feature_importances}).sort_values('importance', 
                                                                            ascending = False)
    
    # Find the features with zero importance
    zero_features = list(feature_importances[feature_importances['importance'] == 0.0]['feature'])
    print('\nThere are %d features with 0.0 importance' % len(zero_features))
    
    return zero_features, feature_importances

Y_labels = np.array(Y)
 zero_features, feature_importances = identify_zero_importance_features(X_featured,  Y_labels, iterations = 2)

print(zero_features)

feature_importances.to_csv('/content/drive/My Drive/BUs/feature_importances.csv')

feature_importances = pd.read_csv('/content/drive/My Drive/BUs/feature_importances.csv', index_col=0)

feature_importances["cumsum"]=np.cumsum(feature_importances["importance"])/feature_importances["importance"].sum()
most_important_features=feature_importances[feature_importances["cumsum"]<=0.97]
most_important_features.head(10)

plt.bar(most_important_features["feature"],most_important_features["importance"])
plt.xlabel("Features")
plt.ylabel("Feature importance")
plt.title("Feautre importance - Time to Sell Prediction")

X_final = X_featured.loc[:,most_important_features.iloc[:,1]]
X_final.head()

"""# First Exploration - Days to Sell

## First Model - Linear Regression
"""

# Split Train/Test
# Keep 20% of the data for testing.
X_train, X_test, Y_train, Y_test = train_test_split(
    X_final, Y, test_size=0.2, random_state=1234, shuffle=True)

# Commented out IPython magic to ensure Python compatibility.
from sklearn import datasets, linear_model
from sklearn.metrics import mean_squared_error, r2_score

# Create linear regression object
reg = linear_model.LinearRegression()
print(reg)

# Train the model using the training sets
reg.fit(X_train, Y_train)

# Make train predictions
Y_train_pred = reg.predict(X_train)

print("Train Root Mean squared error: %.3f"
#       % mean_squared_error(Y_train, Y_train_pred)**0.5)

# Make test predictions
Y_test_pred = reg.predict(X_test)

print("Test Root Mean squared error: %.3f"
#      % mean_squared_error(Y_test, Y_test_pred)**0.5)

Y_test.head()

Y_test_pred = pd.DataFrame(Y_test_pred)
Y_comb_test = pd.concat([Y_test.reset_index(), Y_test_pred], axis=1, ignore_index=False)
Y_comb_test.rename({"daystosell": "daystosell",  
           0: "daystosell_pred"},  
          axis = "columns", inplace = True) 

Y_comb_test.head()

plt.scatter(Y_comb_test["daystosell"],Y_comb_test["daystosell_pred"])
plt.xlabel("Days for a house to be sold") 
plt.ylabel("Predicted - Days to sell a house")

"""We have **negative values** !!! --> we should not use Linear Regression

## Second Model - XGBoost reg:linear
"""

# Commented out IPython magic to ensure Python compatibility.
import xgboost as xgb

# Create linear regression objec
xgbr = xgb.XGBRegressor(verbosity=0)
print(xgbr)

# Train the model using the training sets
xgbr.fit(X_train, Y_train)


# Make train predictions
Y_train_pred = xgbr.predict(X_train)

print("Train Root Mean squared error: %.3f"
#       % mean_squared_error(Y_train, Y_train_pred)**0.5)

# Make test predictions
Y_test_pred = xgbr.predict(X_test)

print("Test Root Mean squared error: %.3f"
#      % mean_squared_error(Y_test, Y_test_pred)**0.5)

Y_test_pred = pd.DataFrame(Y_test_pred)
Y_comb_test = pd.concat([Y_test.reset_index(), Y_test_pred], axis=1, ignore_index=False)
Y_comb_test.rename({"daystosell": "daystosell",  
           0: "daystosell_pred"},  
          axis = "columns", inplace = True)

plt.scatter(Y_comb_test["daystosell"],Y_comb_test["daystosell_pred"])
plt.xlabel("Days for a house to be sold") 
plt.ylabel("Predicted - Days to sell a house")

"""# Hyper Parameter Optimization - Days to Sell

## Feature importance Selection
"""

importance_scalar = [0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99]

def feature_selection (importance_scalar,feature_importances,X_featured, Y):

  pred_dict = list()

  for i in importance_scalar:
    # selection of features
    most_important_features=feature_importances[feature_importances["cumsum"]<=i]
    X_final = X_featured.loc[:,most_important_features.iloc[:,1]]

    # train & test split
    X_train, X_test, Y_train, Y_test = train_test_split(
        X_final, Y, test_size=0.2, random_state=1234, shuffle=True)
    
    # Create linear regression objec
    xgbr = xgb.XGBRegressor(verbosity=0)

    # Train the model using the training sets
    xgbr.fit(X_train, Y_train)

    # Make test predictions
    Y_test_pred = xgbr.predict(X_test)

    pred_dict.append({"Importance_Param":i,"Y_test_error": mean_squared_error(Y_test, Y_test_pred)**0.5})

  return (pred_dict)

pred_dict_importance_param = feature_selection (importance_scalar,feature_importances,X_featured, Y)

print(pd.DataFrame(pred_dict_importance_param))

"""## Feature Type"""

feature_to_remove = ["List Price","exp","cluster_public_remarks","pca_public_remarks","pca_view","cluster_view","postalcode3"]

def feature_impact (feature_to_remove,X_train,Y_train,X_test,Y_test):

  pred_dict = list()

  for i in feature_to_remove:    
    # Create linear regression objec
    xgbr = xgb.XGBRegressor(verbosity=0)

    # features to remove
    X_train_bis = X_train [X_train.columns.drop(list(X_train.filter(regex=i)))]
    X_test_bis = X_test [X_test.columns.drop(list(X_test.filter(regex=i)))]

    # Train the model using the training sets
    xgbr.fit(X_train_bis, Y_train)

    # Make test predictions
    Y_test_pred = xgbr.predict(X_test_bis)

    pred_dict.append({"Removed Feature":i,"Y_test_error": mean_squared_error(Y_test, Y_test_pred)**0.5})

  return (pred_dict)

pred_dict_feature_to_remove = feature_impact (feature_to_remove,X_train,Y_train,X_test,Y_test)

print(pd.DataFrame(pred_dict_feature_to_remove))

"""## Learning Rate Selection"""

learning_rate = [0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4]

def learning_rate_selection (learning_rate,X_train,Y_train,X_test,Y_test):

  pred_dict = list()

  for i in learning_rate:    
    # Create linear regression objec
    xgbr = xgb.XGBRegressor(verbosity=0, learning_rate=i)


    # Train the model using the training sets
    xgbr.fit(X_train, Y_train)

    # Make test predictions
    Y_test_pred = xgbr.predict(X_test)

    pred_dict.append({"Eta_Param":i,"Y_test_error": mean_squared_error(Y_test, Y_test_pred)**0.5})

  return (pred_dict)

pred_dict_learning_rate = learning_rate_selection (learning_rate,X_train,Y_train,X_test,Y_test)

print(pd.DataFrame(pred_dict_learning_rate))

"""## Gamma Selection"""

gamma = [0,1,10,100,1000]

def gamma_selection (gamma,X_train,Y_train,X_test,Y_test):

  pred_dict = list()

  for i in gamma:    
    # Create linear regression objec
    xgbr = xgb.XGBRegressor(verbosity=0, learning_rate=0.30, gamma = i)

    # Train the model using the training sets
    xgbr.fit(X_train, Y_train)

    # Make test predictions
    Y_test_pred = xgbr.predict(X_test)

    pred_dict.append({"Gamma_Param":i,"Y_test_error": mean_squared_error(Y_test, Y_test_pred)**0.5})

  return (pred_dict)

pred_dict_gamma = gamma_selection (gamma,X_train,Y_train,X_test,Y_test)

print(pd.DataFrame(pred_dict_gamma))

"""## Max Depth Selection"""

max_depth = [1,2,3,4,5,6]

def max_depth_selection (max_depth,X_train,Y_train,X_test,Y_test):

  pred_dict = list()

  for i in max_depth:    
    # Create linear regression objec
    xgbr = xgb.XGBRegressor(verbosity=0, learning_rate=0.30, max_depth = i)

    # Train the model using the training sets
    xgbr.fit(X_train, Y_train)

    # Make test predictions
    Y_test_pred = xgbr.predict(X_test)

    pred_dict.append({"max_depth":i,"Y_test_error": mean_squared_error(Y_test, Y_test_pred)**0.5})

  return (pred_dict)

pred_dict_max_depth = max_depth_selection (max_depth,X_train,Y_train,X_test,Y_test)

print(pd.DataFrame(pred_dict_max_depth))

"""## N rounds Selection"""

n_estimators = [50,60,70,80,90,100]

def n_estimators_selection (n_estimators,X_train,Y_train,X_test,Y_test):

  pred_dict = list()

  for i in n_estimators:    
    # Create linear regression objec
    xgbr = xgb.XGBRegressor(verbosity=0, learning_rate=0.30, max_depth = 4, n_estimators=i)

    # Train the model using the training sets
    xgbr.fit(X_train, Y_train)

    # Make test predictions
    Y_test_pred = xgbr.predict(X_test)

    pred_dict.append({"n_estimators":i,"Y_test_error": mean_squared_error(Y_test, Y_test_pred)**0.5})

  return (pred_dict)

pred_dict_n_estimators = n_estimators_selection (n_estimators,X_train,Y_train,X_test,Y_test)

print(pd.DataFrame(pred_dict_n_estimators))